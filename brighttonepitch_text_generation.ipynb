{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q torch transformers sentencepiece protobuf gradio\n\nimport gradio as gr\n\nimport torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INFERENCE_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_PATH = 'openlm-research/open_llama_3b_v2'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\nmodel = LlamaForCausalLM.from_pretrained(\n    MODEL_PATH, torch_dtype=torch.float16, device_map=INFERENCE_DEVICE,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text_block(project_description: str, output_type: str) -> dict:\n    prompts_by_output_type = {\n        \"problem\": \"answer, what is the key problem this service solves?\",\n        \"target_audience\": \"answer, what is the target audience of this service?\",\n        \"value_proposition\": \"answer, what is the value proposiion of this service?\",\n        \"description\": \"summarise the description of this service\",\n        \"solution\": \"describe, how the service works\",\n        \"competitors\": \"point its key competitors\",\n        \"founders\": \"point, who are its founders and investors?\",\n        \"investments_direction\": \"imagine, what features can be added to service?\",\n        \"roadmap\": \"imagine the roadmap of service development\"\n    }\n    prompt = f\"Text: {project_description}\\nGiven this information, {prompts_by_output_type[output_type]}\\n\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(INFERENCE_DEVICE)\n\n    generation_output = model.generate(\n        input_ids=input_ids, max_new_tokens=32\n    )\n    return tokenizer.decode(generation_output[0])[len(prompt)+2:].rstrip('</s>')\n\n\ndef generate_pitch_deck(project_description: str, custom_feature: bool):\n    output_types = [\n        'problem', \n        'target_audience', \n        'value_proposition', \n        'description', \n        'solution', \n        'competitors', \n        'founders', \n        'investments_direction', \n        'roadmap'\n    ]\n    project_description = project_description.rstrip()\n    pitch_deck_text_blocks = {\n        output_type: generate_text_block(project_description, output_type) \n        for output_type in output_types\n    }\n    \n    if custom_feature:\n        pass\n    \n    return \"Done\"\n    \n    \ndemo = gr.Interface(\n    fn = generate_pitch_deck,\n    inputs = [\n        gr.Textbox(lines=2, placeholder=\"Project Description Here...\"),\n        gr.Checkbox(label=\"Some custom feature\")\n    ],\n    outputs = \"file\",\n)\ndemo.launch()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}